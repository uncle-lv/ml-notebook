{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_set():\n",
    "    posting_list = [\n",
    "        ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "        ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "        ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "        ['stop', 'posting', 'stupid', 'worthless', 'gar e'],\n",
    "        ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "        ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']\n",
    "    ]\n",
    "\n",
    "    class_vec = [0, 1, 0, 1, 0, 1]\n",
    "    \n",
    "    return posting_list, class_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
       "  ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
       "  ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
       "  ['stop', 'posting', 'stupid', 'worthless', 'gar e'],\n",
       "  ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
       "  ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']],\n",
       " [0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocal_list(dataset):\n",
    "    vocab_set = set()\n",
    "    for item in dataset:\n",
    "        vocab_set = vocab_set | set(item)\n",
    "    \n",
    "    return list(vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
      " ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
      " ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
      " ['stop', 'posting', 'stupid', 'worthless', 'gar e'],\n",
      " ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
      " ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n"
     ]
    }
   ],
   "source": [
    "list_post, list_classes = load_data_set()\n",
    "pprint(list_post)\n",
    "vocab_list = create_vocal_list(list_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_of_words2vec(vocab_list, input_set):\n",
    "    result = [0] * len(vocab_list)\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            result[vocab_list.index(word)] = 1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0,\n",
      "  1,\n",
      "  0,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  0,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0],\n",
      " [0,\n",
      "  0,\n",
      "  1,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  0,\n",
      "  1,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  0,\n",
      "  0],\n",
      " [1,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  0,\n",
      "  1,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  0],\n",
      " [0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  1,\n",
      "  0,\n",
      "  0],\n",
      " [0,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  0,\n",
      "  1,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  0],\n",
      " [0,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  1,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  0,\n",
      "  1,\n",
      "  0,\n",
      "  1]]\n"
     ]
    }
   ],
   "source": [
    "train_mat = []\n",
    "for post_in in list_post:\n",
    "    train_mat.append(\n",
    "        set_of_words2vec(vocab_list, post_in)\n",
    "    )\n",
    "pprint(train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_naive_bayes(train_mat, train_category):\n",
    "    train_doc_num = len(train_mat)\n",
    "    words_num = len(train_mat[0])\n",
    "    pos__abusive = np.sum(train_category) / train_doc_num\n",
    "    p0num = np.zeros(words_num)\n",
    "    p1num = np.zeros(words_num)\n",
    "\n",
    "    p0num_all = 0\n",
    "    p1num_all = 0\n",
    "\n",
    "    for i in range(train_doc_num):\n",
    "        if train_category[i] == 1:\n",
    "            p1num += train_mat[i]\n",
    "            p1num_all += np.sum(train_mat[i])\n",
    "        else:\n",
    "            p0num += train_mat[i]\n",
    "            p0num_all += np.sum(train_mat[i])\n",
    "    \n",
    "    p0vec = p0num / p0num_all\n",
    "    p1vec = p1num / p1num_all\n",
    "\n",
    "    return p0vec, p1vec, pos__abusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(train_mat, train_category):\n",
    "    train_doc_num = len(train_mat)\n",
    "    words_num = len(train_mat[0])\n",
    "    pos_absuive = np.sum(train_category) / train_doc_num\n",
    "\n",
    "    p0num = np.ones(words_num)\n",
    "    p1num = np.ones(words_num)\n",
    "    p0num_all = 2.0\n",
    "    p1num_all = 2.0\n",
    "    for i in range(train_doc_num):\n",
    "        if train_category[i] == 1:\n",
    "            p1num += train_mat[i]\n",
    "            p1num_all += np.sum(train_mat[i])\n",
    "        else:\n",
    "            p0num += train_mat[i]\n",
    "            p0num_all += np.sum(train_mat[i])\n",
    "    \n",
    "    p0vec = np.log(p0num / p0num_all)\n",
    "    p1vec = np.log(p1num / p1num_all)\n",
    "    \n",
    "    return p0vec, p1vec, pos_absuive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_naive_bayes(vec2classify, p0vec, p1vec, p_class1):\n",
    "    p0 = np.sum(vec2classify * p0vec) + np.log(1 - p_class1)\n",
    "    p1 = np.sum(vec2classify * p1vec) + np.log(p_class1)\n",
    "    if p0 > p1:\n",
    "        return 0\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_words2vec(vocab_list, input_set):\n",
    "    result = [0] * len(vocab_list)\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            result[vocab_list.index(word)] += 1\n",
    "        else:\n",
    "            pprint(f'the word: {word} is not in my vocabulary')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_naive_bayes():\n",
    "    list_post, list_classes = load_data_set()\n",
    "    vocab_list = create_vocal_list(list_post)\n",
    "\n",
    "    train_mat = []\n",
    "    for post_in in list_post:\n",
    "        train_mat.append(\n",
    "            set_of_words2vec(vocab_list, post_in)\n",
    "        )\n",
    "\n",
    "    p0v, p1v, p_abusive = train_naive_bayes(np.array(train_mat), np.array(list_classes))\n",
    "    test_one = ['love', 'my', 'dalmation']\n",
    "    test_one_doc = np.array(set_of_words2vec(vocab_list, test_one))\n",
    "    pprint(f'the result is: {classify_naive_bayes(test_one_doc, p0v, p1v, p_abusive)}')\n",
    "    test_two = ['stupid', 'garbage']\n",
    "    test_two_doc = np.array(set_of_words2vec(vocab_list, test_two))\n",
    "    pprint(f'the result is: {classify_naive_bayes(test_two_doc, p0v, p1v, p_abusive)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'the result is: 0'\n",
      "'the result is: 1'\n"
     ]
    }
   ],
   "source": [
    "testing_naive_bayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_parse(big_str):\n",
    "    import re\n",
    "\n",
    "    token_list = re.split(r'\\W+', big_str)\n",
    "    if len(token_list) == 0:\n",
    "        pprint(token_list)\n",
    "\n",
    "    return [ tok.lower() for tok in token_list if len(tok) > 2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def spam_test():\n",
    "#     doc_list = []\n",
    "#     class_list = []\n",
    "#     full_text = []\n",
    "#     for i in range(1, 25):\n",
    "#         try:\n",
    "#             words = text_parse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99d00b958b0c31c377523ff4d19507b43206f3bc6ac57ab1857d0564bf375d13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
